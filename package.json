{
  "name": "llama-coder",
  "displayName": "Llama Coder",
  "description": "Better and self-hosted Github Copilot replacement",
  "version": "0.0.14",
  "icon": "icon.png",
  "publisher": "ex3ndr",
  "repository": {
    "type": "git",
    "url": "https://github.com/ex3ndr/llama-coder.git"
  },
  "bugs": {
    "url": "https://github.com/ex3ndr/llama-coder/issues"
  },
  "license": "MIT",
  "categories": [
    "Machine Learning",
    "Programming Languages"
  ],
  "keywords": [
    "code",
    "assistant",
    "ai",
    "llm",
    "development"
  ],
  "engines": {
    "vscode": "^1.84.0"
  },
  "activationEvents": [
    "onStartupFinished"
  ],
  "extensionKind": [
    "ui"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "llama.openSettings",
        "title": "Llama Coder: Open Settings"
      },
      {
        "command": "llama.pause",
        "title": "Llama Coder: Pause"
      },
      {
        "command": "llama.resume",
        "title": "Llama Coder: Resume"
      },
      {
        "command": "llama.toggle",
        "title": "Llama Coder: Toggle"
      }
    ],
    "configuration": [
      {
        "title": "Llama coder",
        "properties": {
          "notebook.includeMarkup": {
            "type": "boolean",
            "default": true,
            "description": "Include markup cell types in prompt"
          },
          "notebook.includeCellOutputs": {
            "type": "boolean",
            "default": false,
            "description": "Include Cell previous output results in the prompt"
          },
          "notebook.cellOutputLimit": {
            "type": "number",
            "default": 256,
            "description": "truncate cell output result if exceeds this limit"
          },
          "inference.endpoint": {
            "type": "string",
            "default": "",
            "description": "Ollama Server Endpoint. Empty for local instance. Example: http://192.168.0.100:11434",
            "order": 1
          },
          "inference.bearerToken": {
            "type": "string",
            "default": "",
            "description": "Auth Bearer token that should be used for secure requests. Leave empty if not desired."
          },
          "inference.model": {
            "type": "string",
            "enum": [
              "deepseek-coder:1.3b-base-q4_0",
              "deepseek-coder:1.3b-base-q4_1",
              "deepseek-coder:1.3b-base-q8_0",
              "deepseek-coder:6.7b-base-q4_K_S",
              "deepseek-coder:6.7b-base-q4_K_M",
              "deepseek-coder:6.7b-base-q5_K_S",
              "deepseek-coder:6.7b-base-q5_K_M",
              "deepseek-coder:6.7b-base-q8_0",
              "deepseek-coder:6.7b-base-fp16",
              "deepseek-coder:33b-base-q4_K_S",
              "deepseek-coder:33b-base-q4_K_M",
              "deepseek-coder:33b-base-fp16",
              "qwen2.5-coder:0.5b",
              "qwen2.5-coder:1.5b",
              "qwen2.5-coder:3b",
              "qwen2.5-coder:7b",
              "qwen2.5-coder:14b",
              "qwen2.5-coder:32b",
              "qwen3-coder:30b",
              "custom"
            ],
            "default": "qwen2.5-coder:7b",
            "description": "Inference model to use. DeepSeek Coder models trained on 2T tokens (87% code). Qwen2.5-Coder models trained on massive datasets with 32K context window (40+ languages). Qwen3-Coder:30b uses MoE architecture with 256K context.",
            "order": 2
          },
          "inference.temperature": {
            "type": "number",
            "default": 0.2,
            "minimum": 0,
            "maximum": 2,
            "description": "Sampling temperature (0-2). Lower values (0.1-0.3) produce more deterministic completions, higher values (0.7-1.0) produce more creative/varied output. Recommended: 0.2-0.3 for code completion.",
            "order": 3
          },
          "inference.custom.model": {
            "type": "string",
            "default": "",
            "description": "Custom model name (e.g., 'qwen2.5-coder:7b'). Only used when 'inference.model' is set to 'custom'. Must match the exact model name in Ollama.",
            "order": 4
          },
          "inference.custom.format": {
            "type": "string",
            "enum": [
              "deepseek",
              "qwen"
            ],
            "default": "qwen",
            "description": "Fill-In-Middle (FIM) prompt format for custom model. Must match the format your model was trained with: 'deepseek' for DeepSeek Coder, 'qwen' for Qwen2.5-Coder and Qwen3-Coder. Using wrong format will produce poor results.",
            "order": 5
          },
          "inference.maxLines": {
            "type": "number",
            "default": 16,
            "minimum": 1,
            "maximum": 100,
            "description": "Maximum number of lines to generate before stopping (when at top-level scope). Controls completion length. Recommended: 16-30. Higher values allow longer multi-line completions but may be slower.",
            "order": 6
          },
          "inference.maxTokens": {
            "type": "number",
            "default": 256,
            "minimum": 10,
            "maximum": 4096,
            "description": "Maximum number of tokens (word pieces) the model will generate. Controls inference cost and completion length. Recommended: 256-512 for single completions, 1024+ for comprehensive multi-line suggestions. Too low (e.g., 5) will produce incomplete results.",
            "order": 7
          },
          "inference.delay": {
            "type": "number",
            "default": 250,
            "minimum": -1,
            "maximum": 5000,
            "description": "Debounce delay in milliseconds before triggering completion after typing stops. Lower values (100-250ms) give faster suggestions but more API calls. Higher values (500-1000ms) reduce API load. Set to -1 to disable completions entirely. Set to 0 for immediate suggestions (not recommended).",
            "order": 8
          },
          "inference.maxSuffixLength": {
            "type": "number",
            "default": 1000,
            "minimum": 0,
            "maximum": 50000,
            "description": "Maximum characters of code after cursor to include as context. Lower values (500-2000) help model focus on immediate completion, reducing hallucinations. Higher values (5000+) provide more context but may confuse the model. Set to 0 to disable suffix context entirely (non-FIM mode). Recommended: 1000-2000 for best results.",
            "order": 9
          },
          "inference.maxPrefixLength": {
            "type": "number",
            "default": 10000,
            "minimum": 100,
            "maximum": 100000,
            "description": "Maximum characters of code before cursor to include as context. Prefix (history) is generally more useful than suffix. Higher values provide more context but increase prompt size and inference time. Recommended: 10000-20000.",
            "order": 10
          }
        }
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "yarn run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "pretest": "yarn run compile && yarn run lint",
    "lint": "eslint src --ext ts",
    "test": "jest",
    "package": "npx @vscode/vsce package"
  },
  "devDependencies": {
    "@types/jest": "^29.5.10",
    "@types/node": "18.x",
    "@types/vscode": "^1.84.0",
    "@typescript-eslint/eslint-plugin": "^6.9.0",
    "@typescript-eslint/parser": "^6.9.0",
    "dotenv": "^16.3.1",
    "eslint": "^8.52.0",
    "jest": "^29.7.0",
    "ts-jest": "^29.1.1",
    "typescript": "^5.2.2"
  },
  "packageManager": "yarn@1.22.22+sha512.a6b2f7906b721bba3d67d4aff083df04dad64c399707841b7acf00f6b133b7ac24255f2652fa22ae3534329dc6180534e98d17432037ff6fd140556e2bb3137e"
}
